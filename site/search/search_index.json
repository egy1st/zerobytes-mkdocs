{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ZeroBytes.One If you are asking yourself, why this website called zero bytes, then let me tell you a secret \"I just create zeros to give you the power to create complete bytes\". In a nutshell, Zero Bytes is a journey from 0 ==> 1, from NULL to totally Full, from no thing to som e thing, from 0 000 0000 ==> 0000 000 1 What Will You Get From These Bytes? Keep up with the future with in-demand data science knowledge. Top companies need data science skills. Learn at your own pace, Where Free means Premium. Python: A complete Byte MS Excel: step by step DenMune: Density-based clustering DenMune: Documentation MS Excel: Interactive Training Counting in 13 Languages including Arabic, English & Chinese ZeroBytes: When less means more \"Obviously, there is a trade-off between gaining knowledge and the costly time and money to obtain these knowledge. In ZeroBytes we believe that knowledge should be available to learner in premium, but at no cost.\" \"When Free means Premium. When less means More. This is the most complex formula we master in ZeroBytes. We just started.. There are many things that seem impossible only so long as one does not attempt them.\" \"Our mission is to make the Arab world a better place to learn, gain knowledge and apply these knowledge to solve real-life problems that is result in leading a better life.\" Please see the projects license for further details.","title":"Home"},{"location":"#welcome-to-zerobytesone","text":"If you are asking yourself, why this website called zero bytes, then let me tell you a secret \"I just create zeros to give you the power to create complete bytes\". In a nutshell, Zero Bytes is a journey from 0 ==> 1, from NULL to totally Full, from no thing to som e thing, from 0 000 0000 ==> 0000 000 1","title":"Welcome to ZeroBytes.One"},{"location":"#what-will-you-get-from-these-bytes","text":"Keep up with the future with in-demand data science knowledge. Top companies need data science skills. Learn at your own pace, Where Free means Premium. Python: A complete Byte MS Excel: step by step DenMune: Density-based clustering DenMune: Documentation MS Excel: Interactive Training Counting in 13 Languages including Arabic, English & Chinese","title":"What Will You Get From These Bytes?"},{"location":"#zerobytes-when-less-means-more","text":"\"Obviously, there is a trade-off between gaining knowledge and the costly time and money to obtain these knowledge. In ZeroBytes we believe that knowledge should be available to learner in premium, but at no cost.\" \"When Free means Premium. When less means More. This is the most complex formula we master in ZeroBytes. We just started.. There are many things that seem impossible only so long as one does not attempt them.\" \"Our mission is to make the Arab world a better place to learn, gain knowledge and apply these knowledge to solve real-life problems that is result in leading a better life.\" Please see the projects license for further details.","title":"ZeroBytes: When less means more"},{"location":"about/","text":"About Me: Mohamed Ali Abbas Experienced Data Analyst with a demonstrated history of working in the government administration industry. Skilled in Python, C++ and Statistical Data Analysis. strong information technology professional with a MSc in Computer Science and PhD in Information Technology. Focused in data mining, machine learning and pattern recognition. A Recognized Instructor in many reputable organizations.","title":"About"},{"location":"about/#about-me-mohamed-ali-abbas","text":"Experienced Data Analyst with a demonstrated history of working in the government administration industry. Skilled in Python, C++ and Statistical Data Analysis. strong information technology professional with a MSc in Computer Science and PhD in Information Technology. Focused in data mining, machine learning and pattern recognition. A Recognized Instructor in many reputable organizations.","title":"About Me: Mohamed Ali Abbas"},{"location":"license/","text":"BSD 3-Clause License Copyright (c) [2021], [Mohamed Ali Abbas] Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD 3-Clause License"},{"location":"license/#bsd-3-clause-license","text":"Copyright (c) [2021], [Mohamed Ali Abbas] Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD 3-Clause License"},{"location":"Certificates/","text":"Am I Certified? List of my Obtained Certificates: IBM-Data-Science-Professional-Certificate","title":"Certificates"},{"location":"Certificates/#am-i-certified","text":"List of my Obtained Certificates: IBM-Data-Science-Professional-Certificate","title":"Am I Certified?"},{"location":"Certificates/IBM/","text":"IBM-Data-Science-Professional-Certificate Back in January, 2019, I did these projects many years ago as a requirement to obtain my IBM Data Science Professional Certificate. About This Certificate Certificate name: Data Science Professional Certificate Certificate URL: https://coursera.org/share/5cf76b4a724b6f51c8aafdcdd1ed8396 Issued by Coursera Authorized by IBM I have been awarded 13 badges on obtaining this certificate https://www.credly.com/users/mohamed-abbas.1f43f901/badges https://www.credly.com/org/ibm/badge/data-science-professional-certificate-v2 What Credly tells about me The badge earner is ready for a career in data science with demonstrated ability to solve for real-world problems. They can apply Data Science methodology - work with Jupyter notebooks - create Python apps - access relational databases using SQL & Python - use Python libraries to generate data visualizations - perform data analysis using Pandas - construct & evaluate Machine Learning (ML) models using Scikit-learn & SciPy and apply data science & ML techniques to real location data sets. Earning Criteria Complete all courses in the IBM Data Science Professional Certificate program on Coursera (includes quizzes, hands-on assignments and projects), and earn the following badges: Data Science Orientation Tools for Data Science Data Science Methodology Python for Data Science and AI Databases and SQL for Data Science Data Visualization with Python Data Analysis with Python Machine Learning with Python Applied Data Science Capstone Python Project for Data Science Receive the Data Science Professional Certificate from Coursera. Haga clic aqu\u00ed para la versi\u00f3n en Espa\u00f1ol: Ciencia de Datos de IBM Professional Certificate Endorsements American Council on Education CREDIT This credential has been successfully evaluated by the American Council on Education for college credit. It is recommended for a total of 12 college credits. For more information about ACE Learning Evaluations, visit www.acenet.edu. Skills AI Artificial Intelligence Bokeh Classification Clustering Data Analysis Database Data Science Data Visualization Db2 Folium Foursquare IBM Cloud Jupyter Location Machine Learning Matplotlib Methodology ML Notebook NumPy Pandas Python Recommender Systems Regression RStudio Scikit-learn SciPy Seaborn SQL Studio Watson Zeppelin Certificate Verification https://www.coursera.org/account/accomplishments/specialization/certificate/7ECY2S6SQBEJ https://www.credly.com/badges/3bcd7d62-335a-462f-9075-68e17e96ae12","title":"IBM-Data-Science-Professional-Certificate"},{"location":"Certificates/IBM/#ibm-data-science-professional-certificate","text":"Back in January, 2019, I did these projects many years ago as a requirement to obtain my IBM Data Science Professional Certificate.","title":"IBM-Data-Science-Professional-Certificate"},{"location":"Certificates/IBM/#about-this-certificate","text":"Certificate name: Data Science Professional Certificate Certificate URL: https://coursera.org/share/5cf76b4a724b6f51c8aafdcdd1ed8396 Issued by Coursera Authorized by IBM","title":"About This Certificate"},{"location":"Certificates/IBM/#i-have-been-awarded-13-badges-on-obtaining-this-certificate","text":"https://www.credly.com/users/mohamed-abbas.1f43f901/badges https://www.credly.com/org/ibm/badge/data-science-professional-certificate-v2","title":"I have been awarded 13 badges on obtaining this certificate"},{"location":"Certificates/IBM/#what-credly-tells-about-me","text":"The badge earner is ready for a career in data science with demonstrated ability to solve for real-world problems. They can apply Data Science methodology - work with Jupyter notebooks - create Python apps - access relational databases using SQL & Python - use Python libraries to generate data visualizations - perform data analysis using Pandas - construct & evaluate Machine Learning (ML) models using Scikit-learn & SciPy and apply data science & ML techniques to real location data sets.","title":"What Credly tells about me"},{"location":"Certificates/IBM/#earning-criteria","text":"Complete all courses in the IBM Data Science Professional Certificate program on Coursera (includes quizzes, hands-on assignments and projects), and earn the following badges: Data Science Orientation Tools for Data Science Data Science Methodology Python for Data Science and AI Databases and SQL for Data Science Data Visualization with Python Data Analysis with Python Machine Learning with Python Applied Data Science Capstone Python Project for Data Science Receive the Data Science Professional Certificate from Coursera. Haga clic aqu\u00ed para la versi\u00f3n en Espa\u00f1ol: Ciencia de Datos de IBM Professional Certificate","title":"Earning Criteria"},{"location":"Certificates/IBM/#endorsements","text":"American Council on Education CREDIT This credential has been successfully evaluated by the American Council on Education for college credit. It is recommended for a total of 12 college credits. For more information about ACE Learning Evaluations, visit www.acenet.edu.","title":"Endorsements"},{"location":"Certificates/IBM/#skills","text":"AI Artificial Intelligence Bokeh Classification Clustering Data Analysis Database Data Science Data Visualization Db2 Folium Foursquare IBM Cloud Jupyter Location Machine Learning Matplotlib Methodology ML Notebook NumPy Pandas Python Recommender Systems Regression RStudio Scikit-learn SciPy Seaborn SQL Studio Watson Zeppelin","title":"Skills"},{"location":"Certificates/IBM/#certificate-verification","text":"https://www.coursera.org/account/accomplishments/specialization/certificate/7ECY2S6SQBEJ https://www.credly.com/badges/3bcd7d62-335a-462f-9075-68e17e96ae12","title":"Certificate Verification"},{"location":"ML/","text":"ML Bytes DenMune Clustering Algorithm ReadMe Documentation GitHub","title":"Machine Learning"},{"location":"ML/#ml-bytes","text":"DenMune Clustering Algorithm ReadMe Documentation GitHub","title":"ML Bytes"},{"location":"ML/denmune/","text":"DenMune: A density-peak clustering algorithm DenMune a clustering algorithm that can find clusters of arbitrary size, shapes and densities in two-dimensions. Higher dimensions are first reduced to 2-D using the t-sne. The algorithm relies on a single parameter K (the number of nearest neighbors). The results show the superiority of the algorithm. Enjoy the simplicity but the power of DenMune. Scientific Work Paper Journal Data Coding & Maintenance Git Repo Code Style Installation CI Workflow Code Coverage Docs & Tutorials Read the Docs Repo2Docker Colab kaggle ReviewNB Downloads Stats downloads/day download/week download/month Suggestions & Reporting Issues Github Gitter Slack Based on the paper Paper Mohamed Abbas, Adel El-Zoghabi, Amin Shoukry, DenMune: Density peak based clustering using mutual nearest neighbors In: Journal of Pattern Recognition, Elsevier, volume 109, number 107589, January 2021 DOI: https://doi.org/10.1016/j.patcog.2020.107589 Documentation: Documentation, including tutorials, are available on https://denmune.readthedocs.io [![read the documentation](https://img.shields.io/badge/read_the-docs-orange)](https://denmune.readthedocs.io/en/latest/?badge=latest) Watch it in action This 30 seconds will tell you how a density-based algorithm, DenMune propagates: When less means more Most calssic clustering algorithms fail in detecting complex clusters where clusters are of different size, shape, density, and being exist in noisy data. Recently, a density-based algorithm named DenMune showed great ability in detecting complex shapes even in noisy data. it can detect number of clusters automatically, detect both pre-identified-noise and post-identified-noise automatically and removing them. It can achieve accuracy reach 100% in some classic pattern problems, achieve 97% in MNIST dataset. A great advantage of this algorithm is being single-parameter algorithm. All you need is to set number of k-nearest neighbor and the algorithm will care about the rest. Being Non-sensitive to changes in k, make it robust and stable. Keep in mind, the algorithm reduce any N-D dataset to only 2-D dataset initially, so it is a good benefit of this algorithm is being always to plot your data and explore it which make this algorithm a good candidate for data exploration. Finally, the algorithm comes with neat package for visualizing data, validating it and analyze the whole clustering process. How to install DenMune Simply install DenMune clustering algorithm using pip command from the official Python repository From the shell run the command shell pip install denmune From Jupyter notebook cell run the command ipython3 !pip install denmune How to use DenMune Once DenMune is installed, you just need to import it python from denmune import DenMune ###### Please note that first denmune (the package) in small letters, while the other one(the class itself) has D and M in capital case. Read data There are four possible cases of data: - only train data without labels - only labeled train data - labeled train data in addition to test data without labels - labeled train data in addition to labeled test data ```python #============================================= # First scenario: train data without labels # ============================================ data_path = 'datasets/denmune/chameleon/' dataset = \"t7.10k.csv\" data_file = data_path + dataset # train data without labels X_train = pd.read_csv(data_file, sep=',', header=None) knn = 39 # k-nearest neighbor, the only parameter required by the algorithm dm = DenMune(train_data=X_train, k_nearest=knn) labels, validity = dm.fit_predict(show_analyzer=False, show_noise=True) ``` This is an intuitive dataset which has no groundtruth provided ```python #============================================= # Second scenario: train data with labels # ============================================ data_path = 'datasets/denmune/shapes/' dataset = \"aggregation.csv\" data_file = data_path + dataset # train data with labels X_train = pd.read_csv(data_file, sep=',', header=None) y_train = X_train.iloc[:, -1] X_train = X_train.drop(X_train.columns[-1], axis=1) knn = 6 # k-nearest neighbor, the only parameter required by the algorithm dm = DenMune(train_data=X_train, train_truth= y_train, k_nearest=knn) labels, validity = dm.fit_predict(show_analyzer=False, show_noise=True) ``` Datset groundtruth Dataset as detected by DenMune at k=6 ```python #================================================================= # Third scenario: train data with labels in addition to test data # ================================================================ data_path = 'datasets/denmune/pendigits/' file_2d = data_path + 'pendigits-2d.csv' # train data with labels X_train = pd.read_csv(data_path + 'train.csv', sep=',', header=None) y_train = X_train.iloc[:, -1] X_train = X_train.drop(X_train.columns[-1], axis=1) # test data without labels X_test = pd.read_csv(data_path + 'test.csv', sep=',', header=None) X_test = X_test.drop(X_test.columns[-1], axis=1) knn = 50 # k-nearest neighbor, the only parameter required by the algorithm dm = DenMune(train_data=X_train, train_truth= y_train, test_data= X_test, k_nearest=knn) labels, validity = dm.fit_predict(show_analyzer=True, show_noise=True) ``` dataset groundtruth dataset as detected by DenMune at k=50 test data as predicted by DenMune on training the dataset at k=50 Algorithm's Parameters Parameters used within the initialization of the DenMune class python def __init__ (self, train_data=None, test_data=None, train_truth=None, test_truth=None, file_2d =None, k_nearest=None, rgn_tsne=False, prop_step=0, ): train_data: data used for training the algorithm default: None. It should be provided by the use, otherwise an error will raise. train_truth: labels of training data default: None test_data: data used for testing the algorithm test_truth: labels of testing data default: None k_nearest: number of nearest neighbor default: 0. the default is invalid. k-nearest neighbor should be at least 1. rgn_tsn: when set to True: It will regenerate the reduced 2-D version of the N-D dataset each time the algorithm run. when set to False: It will generate the reduced 2-D version of the N-D dataset first time only, then will reuse the saved exist file default: True file_2d: name (include location) of file used save/load the reduced 2-d version if empty: the algorithm will create temporary file named '_temp_2d' default: None prop_step: size of increment used in showing the clustering propagation. leave this parameter set to 0, the default value, unless you are willing intentionally to enter the propagation mode. default: 0 Parameters used within the fit_predict function: python def fit_predict(self, validate=True, show_plots=True, show_noise=True, show_analyzer=True ): validate: validate data on/off according to five measures integrated with DenMune (Accuracy. F1-score, NMI index, AMI index, ARI index) default: True show_plots: show/hide plotting of data default: True show_noise: show/hide noise and outlier default: True show_analyzer: show/hide the analyzer default: True The Analyzer The algorithm provide an exploratory tool called analyzer, once called it will provide you with in-depth analysis on how your clustering results perform. Noise Detection DenMune detects noise and outlier automatically, no need to any further work from your side. It plots pre-identified noise in black It plots post-identified noise in light grey You can set show_noise parameter to False. ```python # let us show noise m = DenMune(train_data=X_train, k_nearest=knn) labels, validity = dm.fit_predict(show_noise=True) ``` ```python # let us show clean data by removing noise m = DenMune(train_data=X_train, k_nearest=knn) labels, validity = dm.fit_predict(show_noise=False) ``` noisy data clean data Validatation You can get your validation results using 3 methods by showing the Analyzer extract values from the validity returned list from fit_predict function extract values from the Analyzer dictionary - There are five validity measures built-in the algorithm, which are: ACC, Accuracy F1 score NMI index (Normalized Mutual Information) AMI index (Adjusted Mutual Information) ARI index (Adjusted Rand Index) K-nearest Evolution The following chart shows the evolution of pre and post identified noise in correspondence to increase of number of knn. Also, detected number of clusters is analyzed in the same chart in relation with both types of identified noise. The Scalability data size time data size: 5000 time: 2.3139 seconds data size: 10000 time: 5.8752 seconds data size: 15000 time: 12.4535 seconds data size: 20000 time: 18.8466 seconds data size: 25000 time: 28.992 seconds data size: 30000 time: 39.3166 seconds data size: 35000 time: 39.4842 seconds data size: 40000 time: 63.7649 seconds data size: 45000 time: 73.6828 seconds data size: 50000 time: 86.9194 seconds data size: 55000 time: 90.1077 seconds data size: 60000 time: 125.0228 seconds data size: 65000 time: 149.1858 seconds data size: 70000 time: 177.4184 seconds data size: 75000 time: 204.0712 seconds data size: 80000 time: 220.502 seconds data size: 85000 time: 251.7625 seconds data size: 100000 time: 257.563 seconds | The Stability The algorithm is only single-parameter, even more it not sensitive to changes in that parameter, k. You may guess that from the following chart yourself. This is of great benefit for you as a data exploration analyst. You can simply explore the dataset using an arbitrary k. Being Non-sensitive to changes in k, make it robust and stable. Reveal the propagation one of the top performing feature in this algorithm is enabling you to watch how your clusters propagate to construct the final output clusters. just use the parameter 'prop_step' as in the following example: ```python dataset = \"t7.10k\" # data_path = 'datasets/denmune/chameleon/' # train file data_file = data_path + dataset +'.csv' X_train = pd.read_csv(data_file, sep=',', header=None) from itertools import chain # Denmune's Paramaters knn = 39 # number of k-nearest neighbor, the only parameter required by the algorithm # create list of differnt snapshots of the propagation snapshots = chain(range(2,5), range(5,50,10), range(50, 100, 25), range(100,500,100), range(500,2000, 250), range(1000,5500, 500)) from IPython.display import clear_output for snapshot in snapshots: print (\"itration\", snapshot ) clear_output(wait=True) dm = DenMune(train_data=X_train, k_nearest=knn, rgn_tsne=False, prop_step=snapshot) labels, validity = dm.fit_predict(show_analyzer=False, show_noise=False) ``` Interact with the algorithm This notebook allows you interact with the algorithm in many aspects: - you can choose which dataset to cluster (among 4 chameleon datasets) - you can decide which number of k-nearest neighbor to use - show noise on/off; thus you can invesetigate noise detected by the algorithm - show analyzer on/off How to run and test Launch Examples in Repo2Docker Binder Simply use our repo2docker offered by mybinder.org, which encapsulate the algorithm and all required data in one virtual machine instance. All Jupyter notebooks examples found in this repository will be also available to you in action to practice in this respo2docer. Thanks mybinder.org, you made it possible! 2. Launch each Example in Google Research, CoLab Need to test examples one by one, then here another option. Use colab offered by google research to test each example individually. Here is a list of Google CoLab URL to use the algorithm interactively ---------------------------------------------------------------------- Dataset CoLab URL How to use it - colab Chameleon datasets - colab 2D Shape datasets - colab MNIST dataset - colab iris dataset - colab Get 97% by training MNIST dataset - colab Non-groundtruth datasets - colab Noise detection - colab Validation - colab How it propagates - colab Snapshots of propagation - colab Scalability - colab Stability vs number of nearest neighbors - colab k-nearest-evolution - colab 3. Launch each Example in Kaggle workspace If you are a kaggler like me, then Kaggle, the best workspace where data scientist meet, should fit you to test the algorithm with great experience. | Dataset | Kaggle URL | | ---------------------------------------- | ------------------------------------------------------------ | | When less means more - kaggle | [![When less means more - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)]( https://www.kaggle.com/egyfirst/when-less-means-more) | | Non-groundtruth datasets - kaggle | [![Non-groundtruth datasets](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/detecting-non-groundtruth-datasets) | | 2D Shape datasets - kaggle | [![2D Shape datasets - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/detection-of-2d-shape-datasets) | | MNIST dataset kaggle | [![MNIST dataset - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/get-97-using-simple-yet-one-parameter-algorithm) | | Iris dataset kaggle | [![iris dataset - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/denmune-clustering-iris-dataset) | | Training MNIST to get 97% | [![Training MNIST to get 97%](https://kaggle.com/static/images/open-in-kaggle.svg)]( https://www.kaggle.com/egyfirst/training-mnist-dataset-to-get-97) | | Noise detection - kaggle | [![Noise detection - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)]( https://www.kaggle.com/egyfirst/noise-detection) | | Validation - kaggle | [![Validation - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/validate-in-5-built-in-validity-insexes) | | The beauty of propagation - kaggle | [![The beauty of propagation - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/the-beauty-of-clusters-propagation) | | The beauty of propagation part2 - kaggle | [![The beauty of propagation part 2 - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/the-beauty-of-propagation-part2) | | Snapshots of propagation -kaggle | [![The beauty of propagation - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/beauty-of-propagation-part3) | | Scalability kaggle | [![Scalability - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/scalability-vs-speed) | | Stability - kaggle | [![Stability - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/stability-vs-number-of-nearest-neighbor) | | k-nearest-evolution - kaggle | [![k-nearest-evolution - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/k-nearest-evolution) | How to cite ===== If you have used this codebase in a scientific publication and wish to cite it, please use the Journal of Pattern Recognition article Mohamed Abbas McInnes, Adel El-Zoghaby, Amin Ahoukry, *DenMune: Density peak based clustering using mutual nearest neighbors* In: Journal of Pattern Recognition, Elsevier, volume 109, number 107589. January 2021 bib @article{ABBAS2021107589, title = {DenMune: Density peak based clustering using mutual nearest neighbors}, journal = {Pattern Recognition}, volume = {109}, pages = {107589}, year = {2021}, issn = {0031-3203}, doi = {https://doi.org/10.1016/j.patcog.2020.107589}, url = {https://www.sciencedirect.com/science/article/pii/S0031320320303927}, author = {Mohamed Abbas and Adel El-Zoghabi and Amin Shoukry}, keywords = {Clustering, Mutual neighbors, Dimensionality reduction, Arbitrary shapes, Pattern recognition, Nearest neighbors, Density peak}, abstract = {Many clustering algorithms fail when clusters are of arbitrary shapes, of varying densities, or the data classes are unbalanced and close to each other, even in two dimensions. A novel clustering algorithm \u201cDenMune\u201d is presented to meet this challenge. It is based on identifying dense regions using mutual nearest neighborhoods of size K, where K is the only parameter required from the user, besides obeying the mutual nearest neighbor consistency principle. The algorithm is stable for a wide range of values of K. Moreover, it is able to automatically detect and remove noise from the clustering process as well as detecting the target clusters. It produces robust results on various low and high dimensional datasets relative to several known state of the art clustering algorithms.} } Licensing The DenMune algorithm is 3-clause BSD licensed. Enjoy. Task List [x] Update Github with the DenMune sourcode [x] create repo2docker repository [x] Create pip Package [x] create CoLab shared examples [x] create documentation [x] create Kaggle shared examples [x] PEP8 compliant [x] Continuous integration [x] scikit-learn compatible [x] creating unit tests (coverage: 100%) [x] generating API documentation [ ] create conda package","title":"DenMune: A density-peak clustering algorithm"},{"location":"ML/denmune/#denmune-a-density-peak-clustering-algorithm","text":"DenMune a clustering algorithm that can find clusters of arbitrary size, shapes and densities in two-dimensions. Higher dimensions are first reduced to 2-D using the t-sne. The algorithm relies on a single parameter K (the number of nearest neighbors). The results show the superiority of the algorithm. Enjoy the simplicity but the power of DenMune. Scientific Work Paper Journal Data Coding & Maintenance Git Repo Code Style Installation CI Workflow Code Coverage Docs & Tutorials Read the Docs Repo2Docker Colab kaggle ReviewNB Downloads Stats downloads/day download/week download/month Suggestions & Reporting Issues Github Gitter Slack Based on the paper Paper Mohamed Abbas, Adel El-Zoghabi, Amin Shoukry, DenMune: Density peak based clustering using mutual nearest neighbors In: Journal of Pattern Recognition, Elsevier, volume 109, number 107589, January 2021 DOI: https://doi.org/10.1016/j.patcog.2020.107589 Documentation: Documentation, including tutorials, are available on https://denmune.readthedocs.io [![read the documentation](https://img.shields.io/badge/read_the-docs-orange)](https://denmune.readthedocs.io/en/latest/?badge=latest) Watch it in action This 30 seconds will tell you how a density-based algorithm, DenMune propagates: When less means more Most calssic clustering algorithms fail in detecting complex clusters where clusters are of different size, shape, density, and being exist in noisy data. Recently, a density-based algorithm named DenMune showed great ability in detecting complex shapes even in noisy data. it can detect number of clusters automatically, detect both pre-identified-noise and post-identified-noise automatically and removing them. It can achieve accuracy reach 100% in some classic pattern problems, achieve 97% in MNIST dataset. A great advantage of this algorithm is being single-parameter algorithm. All you need is to set number of k-nearest neighbor and the algorithm will care about the rest. Being Non-sensitive to changes in k, make it robust and stable. Keep in mind, the algorithm reduce any N-D dataset to only 2-D dataset initially, so it is a good benefit of this algorithm is being always to plot your data and explore it which make this algorithm a good candidate for data exploration. Finally, the algorithm comes with neat package for visualizing data, validating it and analyze the whole clustering process. How to install DenMune Simply install DenMune clustering algorithm using pip command from the official Python repository From the shell run the command shell pip install denmune From Jupyter notebook cell run the command ipython3 !pip install denmune How to use DenMune Once DenMune is installed, you just need to import it python from denmune import DenMune ###### Please note that first denmune (the package) in small letters, while the other one(the class itself) has D and M in capital case. Read data There are four possible cases of data: - only train data without labels - only labeled train data - labeled train data in addition to test data without labels - labeled train data in addition to labeled test data ```python #============================================= # First scenario: train data without labels # ============================================ data_path = 'datasets/denmune/chameleon/' dataset = \"t7.10k.csv\" data_file = data_path + dataset # train data without labels X_train = pd.read_csv(data_file, sep=',', header=None) knn = 39 # k-nearest neighbor, the only parameter required by the algorithm dm = DenMune(train_data=X_train, k_nearest=knn) labels, validity = dm.fit_predict(show_analyzer=False, show_noise=True) ``` This is an intuitive dataset which has no groundtruth provided ```python #============================================= # Second scenario: train data with labels # ============================================ data_path = 'datasets/denmune/shapes/' dataset = \"aggregation.csv\" data_file = data_path + dataset # train data with labels X_train = pd.read_csv(data_file, sep=',', header=None) y_train = X_train.iloc[:, -1] X_train = X_train.drop(X_train.columns[-1], axis=1) knn = 6 # k-nearest neighbor, the only parameter required by the algorithm dm = DenMune(train_data=X_train, train_truth= y_train, k_nearest=knn) labels, validity = dm.fit_predict(show_analyzer=False, show_noise=True) ``` Datset groundtruth Dataset as detected by DenMune at k=6 ```python #================================================================= # Third scenario: train data with labels in addition to test data # ================================================================ data_path = 'datasets/denmune/pendigits/' file_2d = data_path + 'pendigits-2d.csv' # train data with labels X_train = pd.read_csv(data_path + 'train.csv', sep=',', header=None) y_train = X_train.iloc[:, -1] X_train = X_train.drop(X_train.columns[-1], axis=1) # test data without labels X_test = pd.read_csv(data_path + 'test.csv', sep=',', header=None) X_test = X_test.drop(X_test.columns[-1], axis=1) knn = 50 # k-nearest neighbor, the only parameter required by the algorithm dm = DenMune(train_data=X_train, train_truth= y_train, test_data= X_test, k_nearest=knn) labels, validity = dm.fit_predict(show_analyzer=True, show_noise=True) ``` dataset groundtruth dataset as detected by DenMune at k=50 test data as predicted by DenMune on training the dataset at k=50 Algorithm's Parameters Parameters used within the initialization of the DenMune class python def __init__ (self, train_data=None, test_data=None, train_truth=None, test_truth=None, file_2d =None, k_nearest=None, rgn_tsne=False, prop_step=0, ): train_data: data used for training the algorithm default: None. It should be provided by the use, otherwise an error will raise. train_truth: labels of training data default: None test_data: data used for testing the algorithm test_truth: labels of testing data default: None k_nearest: number of nearest neighbor default: 0. the default is invalid. k-nearest neighbor should be at least 1. rgn_tsn: when set to True: It will regenerate the reduced 2-D version of the N-D dataset each time the algorithm run. when set to False: It will generate the reduced 2-D version of the N-D dataset first time only, then will reuse the saved exist file default: True file_2d: name (include location) of file used save/load the reduced 2-d version if empty: the algorithm will create temporary file named '_temp_2d' default: None prop_step: size of increment used in showing the clustering propagation. leave this parameter set to 0, the default value, unless you are willing intentionally to enter the propagation mode. default: 0 Parameters used within the fit_predict function: python def fit_predict(self, validate=True, show_plots=True, show_noise=True, show_analyzer=True ): validate: validate data on/off according to five measures integrated with DenMune (Accuracy. F1-score, NMI index, AMI index, ARI index) default: True show_plots: show/hide plotting of data default: True show_noise: show/hide noise and outlier default: True show_analyzer: show/hide the analyzer default: True The Analyzer The algorithm provide an exploratory tool called analyzer, once called it will provide you with in-depth analysis on how your clustering results perform. Noise Detection DenMune detects noise and outlier automatically, no need to any further work from your side. It plots pre-identified noise in black It plots post-identified noise in light grey You can set show_noise parameter to False. ```python # let us show noise m = DenMune(train_data=X_train, k_nearest=knn) labels, validity = dm.fit_predict(show_noise=True) ``` ```python # let us show clean data by removing noise m = DenMune(train_data=X_train, k_nearest=knn) labels, validity = dm.fit_predict(show_noise=False) ``` noisy data clean data Validatation You can get your validation results using 3 methods by showing the Analyzer extract values from the validity returned list from fit_predict function extract values from the Analyzer dictionary - There are five validity measures built-in the algorithm, which are: ACC, Accuracy F1 score NMI index (Normalized Mutual Information) AMI index (Adjusted Mutual Information) ARI index (Adjusted Rand Index) K-nearest Evolution The following chart shows the evolution of pre and post identified noise in correspondence to increase of number of knn. Also, detected number of clusters is analyzed in the same chart in relation with both types of identified noise. The Scalability data size time data size: 5000 time: 2.3139 seconds data size: 10000 time: 5.8752 seconds data size: 15000 time: 12.4535 seconds data size: 20000 time: 18.8466 seconds data size: 25000 time: 28.992 seconds data size: 30000 time: 39.3166 seconds data size: 35000 time: 39.4842 seconds data size: 40000 time: 63.7649 seconds data size: 45000 time: 73.6828 seconds data size: 50000 time: 86.9194 seconds data size: 55000 time: 90.1077 seconds data size: 60000 time: 125.0228 seconds data size: 65000 time: 149.1858 seconds data size: 70000 time: 177.4184 seconds data size: 75000 time: 204.0712 seconds data size: 80000 time: 220.502 seconds data size: 85000 time: 251.7625 seconds data size: 100000 time: 257.563 seconds | The Stability The algorithm is only single-parameter, even more it not sensitive to changes in that parameter, k. You may guess that from the following chart yourself. This is of great benefit for you as a data exploration analyst. You can simply explore the dataset using an arbitrary k. Being Non-sensitive to changes in k, make it robust and stable. Reveal the propagation one of the top performing feature in this algorithm is enabling you to watch how your clusters propagate to construct the final output clusters. just use the parameter 'prop_step' as in the following example: ```python dataset = \"t7.10k\" # data_path = 'datasets/denmune/chameleon/' # train file data_file = data_path + dataset +'.csv' X_train = pd.read_csv(data_file, sep=',', header=None) from itertools import chain # Denmune's Paramaters knn = 39 # number of k-nearest neighbor, the only parameter required by the algorithm # create list of differnt snapshots of the propagation snapshots = chain(range(2,5), range(5,50,10), range(50, 100, 25), range(100,500,100), range(500,2000, 250), range(1000,5500, 500)) from IPython.display import clear_output for snapshot in snapshots: print (\"itration\", snapshot ) clear_output(wait=True) dm = DenMune(train_data=X_train, k_nearest=knn, rgn_tsne=False, prop_step=snapshot) labels, validity = dm.fit_predict(show_analyzer=False, show_noise=False) ``` Interact with the algorithm This notebook allows you interact with the algorithm in many aspects: - you can choose which dataset to cluster (among 4 chameleon datasets) - you can decide which number of k-nearest neighbor to use - show noise on/off; thus you can invesetigate noise detected by the algorithm - show analyzer on/off How to run and test Launch Examples in Repo2Docker Binder Simply use our repo2docker offered by mybinder.org, which encapsulate the algorithm and all required data in one virtual machine instance. All Jupyter notebooks examples found in this repository will be also available to you in action to practice in this respo2docer. Thanks mybinder.org, you made it possible! 2. Launch each Example in Google Research, CoLab Need to test examples one by one, then here another option. Use colab offered by google research to test each example individually. Here is a list of Google CoLab URL to use the algorithm interactively ---------------------------------------------------------------------- Dataset CoLab URL How to use it - colab Chameleon datasets - colab 2D Shape datasets - colab MNIST dataset - colab iris dataset - colab Get 97% by training MNIST dataset - colab Non-groundtruth datasets - colab Noise detection - colab Validation - colab How it propagates - colab Snapshots of propagation - colab Scalability - colab Stability vs number of nearest neighbors - colab k-nearest-evolution - colab 3. Launch each Example in Kaggle workspace If you are a kaggler like me, then Kaggle, the best workspace where data scientist meet, should fit you to test the algorithm with great experience. | Dataset | Kaggle URL | | ---------------------------------------- | ------------------------------------------------------------ | | When less means more - kaggle | [![When less means more - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)]( https://www.kaggle.com/egyfirst/when-less-means-more) | | Non-groundtruth datasets - kaggle | [![Non-groundtruth datasets](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/detecting-non-groundtruth-datasets) | | 2D Shape datasets - kaggle | [![2D Shape datasets - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/detection-of-2d-shape-datasets) | | MNIST dataset kaggle | [![MNIST dataset - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/get-97-using-simple-yet-one-parameter-algorithm) | | Iris dataset kaggle | [![iris dataset - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/denmune-clustering-iris-dataset) | | Training MNIST to get 97% | [![Training MNIST to get 97%](https://kaggle.com/static/images/open-in-kaggle.svg)]( https://www.kaggle.com/egyfirst/training-mnist-dataset-to-get-97) | | Noise detection - kaggle | [![Noise detection - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)]( https://www.kaggle.com/egyfirst/noise-detection) | | Validation - kaggle | [![Validation - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/validate-in-5-built-in-validity-insexes) | | The beauty of propagation - kaggle | [![The beauty of propagation - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/the-beauty-of-clusters-propagation) | | The beauty of propagation part2 - kaggle | [![The beauty of propagation part 2 - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/the-beauty-of-propagation-part2) | | Snapshots of propagation -kaggle | [![The beauty of propagation - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/beauty-of-propagation-part3) | | Scalability kaggle | [![Scalability - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/scalability-vs-speed) | | Stability - kaggle | [![Stability - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/stability-vs-number-of-nearest-neighbor) | | k-nearest-evolution - kaggle | [![k-nearest-evolution - kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/egyfirst/k-nearest-evolution) | How to cite ===== If you have used this codebase in a scientific publication and wish to cite it, please use the Journal of Pattern Recognition article Mohamed Abbas McInnes, Adel El-Zoghaby, Amin Ahoukry, *DenMune: Density peak based clustering using mutual nearest neighbors* In: Journal of Pattern Recognition, Elsevier, volume 109, number 107589. January 2021 bib @article{ABBAS2021107589, title = {DenMune: Density peak based clustering using mutual nearest neighbors}, journal = {Pattern Recognition}, volume = {109}, pages = {107589}, year = {2021}, issn = {0031-3203}, doi = {https://doi.org/10.1016/j.patcog.2020.107589}, url = {https://www.sciencedirect.com/science/article/pii/S0031320320303927}, author = {Mohamed Abbas and Adel El-Zoghabi and Amin Shoukry}, keywords = {Clustering, Mutual neighbors, Dimensionality reduction, Arbitrary shapes, Pattern recognition, Nearest neighbors, Density peak}, abstract = {Many clustering algorithms fail when clusters are of arbitrary shapes, of varying densities, or the data classes are unbalanced and close to each other, even in two dimensions. A novel clustering algorithm \u201cDenMune\u201d is presented to meet this challenge. It is based on identifying dense regions using mutual nearest neighborhoods of size K, where K is the only parameter required from the user, besides obeying the mutual nearest neighbor consistency principle. The algorithm is stable for a wide range of values of K. Moreover, it is able to automatically detect and remove noise from the clustering process as well as detecting the target clusters. It produces robust results on various low and high dimensional datasets relative to several known state of the art clustering algorithms.} } Licensing The DenMune algorithm is 3-clause BSD licensed. Enjoy. Task List [x] Update Github with the DenMune sourcode [x] create repo2docker repository [x] Create pip Package [x] create CoLab shared examples [x] create documentation [x] create Kaggle shared examples [x] PEP8 compliant [x] Continuous integration [x] scikit-learn compatible [x] creating unit tests (coverage: 100%) [x] generating API documentation [ ] create conda package","title":"DenMune: A density-peak clustering algorithm"}]}